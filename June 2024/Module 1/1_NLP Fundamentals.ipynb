{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391af31d",
   "metadata": {},
   "source": [
    "# Day 1 - Content\n",
    "\n",
    "Natural Language Processing Fundamentals\n",
    "\n",
    "1. NLP Introduction\n",
    "2. NLP Stop Words\n",
    "\n",
    "Text Processing Pipeline\n",
    "\n",
    "3. POS Tagging\n",
    "4. Named Entity Recognition\n",
    "5. NLP Statistical Methods - Bag Of Words and TF-IDF\n",
    "\n",
    "Feature Engineering in NLP\n",
    "\n",
    "6. Text Normalization and Tokenization\n",
    "7. Embedding and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60653357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84540303",
   "metadata": {},
   "source": [
    "# 1. NLP Introduction\n",
    "\n",
    "<img src='n4.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a16420",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP\n",
    "- NLU (Natural Language Understanding) - Semantic Analytics(context and intent)\n",
    "- NLG (Natural Language Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistical Modeling\n",
    "\n",
    "- generative modeling -> predicting something based on the probability of previous thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0790a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context\n",
    "\n",
    "\"your t-shirt is killer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8567058",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent\n",
    "\n",
    "\"my mom gave me money to buy 1kg tomotos otherwise she will be angry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67714481",
   "metadata": {},
   "outputs": [],
   "source": [
    "give instructions to a machine\n",
    "- python\n",
    "- java\n",
    "- c\n",
    "\n",
    "chatGPT\n",
    "- english\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "application of NLP\n",
    "\n",
    "- sentiment analysis\n",
    "- toxicity classification - threats, insults, hatred\n",
    "- machine translation\n",
    "- NER - Named Entity Recognition - name, organization, location or quantities\n",
    "- email spam detection\n",
    "- text generation - autocomplete, chatbots, \n",
    "- information retrieval\n",
    "- summarization\n",
    "- question and answering bots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c931f22f",
   "metadata": {},
   "source": [
    "# 2. NLP Stop Words\n",
    "\n",
    "<img src='v41.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc03c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ac884f7",
   "metadata": {},
   "source": [
    "# Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"A\" = 65 # ASCII, utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5afd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Text\n",
    "\n",
    "\"<SUBJECT LINE> Employees details.\\\n",
    "<END><BODY TEXT>Attached are 2 files 1st, one is pairoll 2nd is healtcare !\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7638f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove encoding\n",
    "\n",
    "\"Employees details. Attached are 2 files 1st, one is pairoll 2nd is healtcare !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower casing\n",
    "\n",
    "\"employees details. attached are 2 files 1st, one is pairoll 2nd is healtcare !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# digits to words\n",
    "\n",
    "\"employees details. attached are two files first, one is pairoll second is healtcare !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters - @!#$%^\n",
    "\n",
    "\"employees details attached are two files first one is pairoll second is healtcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b25c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling corrections\n",
    "\n",
    "\"employees details attached are two files first one is payroll second is healthcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3732b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "\n",
    "\"employees details attached two files first one payroll second healthcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eabb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "\n",
    "\"employe detail attach two file first one payroll second healthcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a773c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing - ran - run, jumped - jump\n",
    "\n",
    "\"employe detail attach two file first one payroll second healthcare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f5010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ddc727b",
   "metadata": {},
   "source": [
    "# 3. POS Tagging\n",
    "\n",
    "<img src='n5.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14a6804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b067a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b1dad01",
   "metadata": {},
   "source": [
    "# 4. Named Entity Recognition\n",
    "\n",
    "<img src='v45.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716374b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e3e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d514d0bb",
   "metadata": {},
   "source": [
    "# 5. NLP Statistical Methods - Bag Of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text to numerical embeddings\n",
    "\n",
    "\n",
    "1 - statistical methods\n",
    "        - Bag of Words\n",
    "        - Tf-IDF\n",
    "\n",
    "2 - ML/DL based Methods\n",
    "        - embeddings(lookup) method\n",
    "        - Word2Vec -> Continuous Bag Of Words\n",
    "        - trasformer based architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335eb646",
   "metadata": {},
   "source": [
    "### Bag Of Words\n",
    "\n",
    "<img src='n1.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "the cat sat on the mat\n",
    "the dog sat of the cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbb3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5676fce",
   "metadata": {},
   "source": [
    "### Sequential Representation\n",
    "\n",
    "<img src='n2.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78ea46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdf84ed1",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "- weight each word by its importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Term Frequency - How important is the word in the document?\n",
    "\n",
    "Inverse Document Frequency - How important is the term in the whole corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3269b13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7201b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e8a36d8",
   "metadata": {},
   "source": [
    "# Feature Engineering in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b1907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd29c274",
   "metadata": {},
   "source": [
    "# 6. Text Normalization and Tokenization\n",
    "\n",
    "Tokenization Example - https://platform.openai.com/tokenizer\n",
    "\n",
    "<img src='v42.png' />\n",
    "\n",
    "<img src='v43.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34d9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32276e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'single', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'this is a single sentence.'\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9f8cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'single', 'sentence']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punctuation = [word.lower() for word in tokens if word.isalpha()]\n",
    "no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d421090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is the first sentence.', 'this is the second sentence.', 'this is the document.']\n"
     ]
    }
   ],
   "source": [
    "text = 'this is the first sentence. this is the second sentence. this is the document.'\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57ce1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'the', 'first', 'sentence', '.'], ['this', 'is', 'the', 'second', 'sentence', '.'], ['this', 'is', 'the', 'document', '.']]\n"
     ]
    }
   ],
   "source": [
    "print([word_tokenize(sentence) for sentence in sent_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c195f11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(stop_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c73f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'sentence', '.', 'second', 'sentence', '.', 'document', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'this is the first sentence. this is the second sentence. this is the document.'\n",
    "\n",
    "tokens = [token for token in word_tokenize(text) if token not in stop_words]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ab950",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "1. Word Tokenization\n",
    "2. Sentence Tokenization\n",
    "3. Regular Expression Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6bd22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cb24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cef7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d9a5160",
   "metadata": {},
   "source": [
    "# 7. Embedding and Word2Vec\n",
    "\n",
    "\n",
    "Word2Vec - https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d65d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d2df71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGlove(path):\n",
    "    file = open(path, 'r', encoding='utf8')\n",
    "    model = {}\n",
    "    \n",
    "    for l in file:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        value = np.array([float(val) for val in line[1:]])\n",
    "        model[word] = value\n",
    "    \n",
    "    return model\n",
    "\n",
    "glove = loadGlove('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50a49c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5897  , -0.55043 , -1.0106  ,  0.41226 ,  0.57348 ,  0.23464 ,\n",
       "       -0.35773 , -1.78    ,  0.10745 ,  0.74913 ,  0.45013 ,  1.0351  ,\n",
       "        0.48348 ,  0.47954 ,  0.51908 , -0.15053 ,  0.32474 ,  1.0789  ,\n",
       "       -0.90894 ,  0.42943 , -0.56388 ,  0.69961 ,  0.13501 ,  0.16557 ,\n",
       "       -0.063592,  0.35435 ,  0.42819 ,  0.1536  , -0.47018 , -1.0935  ,\n",
       "        1.361   , -0.80821 , -0.674   ,  1.2606  ,  0.29554 ,  1.0835  ,\n",
       "        0.2444  , -1.1877  , -0.60203 , -0.068315,  0.66256 ,  0.45336 ,\n",
       "       -1.0178  ,  0.68267 , -0.20788 , -0.73393 ,  1.2597  ,  0.15425 ,\n",
       "       -0.93256 , -0.15025 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['python']   # vector embedding for the word Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "700b9c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.92803 ,  0.29096 ,  0.67837 ,  1.0444  , -0.72551 ,  2.1995  ,\n",
       "        0.88767 , -0.94782 ,  0.67426 ,  0.24908 ,  0.95722 ,  0.18122 ,\n",
       "        0.064263,  0.64323 , -1.6301  ,  0.94972 , -0.7367  ,  0.17345 ,\n",
       "        0.67638 ,  0.10026 , -0.033782, -0.76971 ,  0.40519 , -0.099516,\n",
       "        0.79654 ,  0.1103  , -0.076053, -0.090434,  0.015021, -1.137   ,\n",
       "        1.6803  , -0.34424 ,  0.77538 , -1.8718  , -0.17148 ,  0.31956 ,\n",
       "        0.093062,  0.004996,  0.25716 ,  0.52207 , -0.52548 , -0.93144 ,\n",
       "       -1.0553  ,  1.4401  ,  0.30807 , -0.84872 ,  1.9986  ,  0.10788 ,\n",
       "       -0.23633 , -0.17978 ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['neural']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0129d31",
   "metadata": {},
   "source": [
    "### How the system know that these words are similar?\n",
    "\n",
    "- Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf288386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "178263a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92180053]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove['cat'].reshape(1,-1), glove['dog'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f968f0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19825255]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove['cat'].reshape(1,-1), glove['piano'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76946819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7839043]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove['king'].reshape(1,-1), glove['queen'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f09d1",
   "metadata": {},
   "source": [
    "## Words in 2D Embedding Space\n",
    "\n",
    "<img src='v44.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45978ab3",
   "metadata": {},
   "source": [
    "# pipeline for text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe21d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Text Notmalization\n",
    "2. Tokenization\n",
    "3. Tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28885124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Celebration         of Worldcup winning !!\"\n",
    "\n",
    "Text Normalization\n",
    "- lowercasing\n",
    "- puctuation removal(?,!)\n",
    "- trim whitespaces\n",
    "- strip accents\n",
    "- stemming\n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"celebrate of worldcup win\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5629a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization\n",
    "- word tokenization\n",
    "- subword tokenization\n",
    "- character tokenization\n",
    "- sentence tokenization\n",
    "- regular expression tokenization\n",
    "\n",
    "\n",
    "[\"celebrate\", 'of', 'worldcup', 'win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens to IDs\n",
    "- words to numerical values\n",
    "\n",
    "- lookup table -> animal - 18, car - 128\n",
    "- hashing - we create a function that will give you an random ID for a word\n",
    "\n",
    "[35, 21, 4, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903edb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML\n",
    "\n",
    "- feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DL\n",
    "\n",
    "- no need to do feature engineering manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e189a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP\n",
    "\n",
    "close the gap between human lanuage and machine language"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
