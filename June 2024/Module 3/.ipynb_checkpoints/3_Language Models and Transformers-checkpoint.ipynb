{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2171e683",
   "metadata": {},
   "source": [
    "# Language Models and Transformers\n",
    "\n",
    "### Content\n",
    "\n",
    "1. Language Models\n",
    "2. Transformers\n",
    "3. Hugging Face Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8ed6c",
   "metadata": {},
   "source": [
    "# Language Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical model in NLP\n",
    "- understand the structure of human language\n",
    "\n",
    "LM are fundamentals to many NLP tasks\n",
    "\n",
    "- machine translation\n",
    "- speech recognition\n",
    "- information retrieval - semantic similarity between your serach term and potential results\n",
    "                        (https://www.perplexity.ai/)\n",
    "- text generation\n",
    "- sentiment analysis\n",
    "- grammar checking\n",
    "- NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb666a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT - 2017\n",
    "GPT3 - 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732dad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eac43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97608233",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Research paper: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XIWlzBNKjOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eeb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural network architecture\n",
    "\n",
    "it works on attention mechanism\n",
    "\n",
    "\n",
    "\"the inverters in an Indian band. The band was formed in 2021.\"\n",
    "\n",
    "the band -> the inverters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206c041",
   "metadata": {},
   "source": [
    "# Sequence Transduction\n",
    "\n",
    "<img src='t1.gif' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "it is developed to solve a problem called sequence transduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb0f85",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "<img src='t2.gif' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "the problem of long term dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416cb02",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)\n",
    "\n",
    "<img src='t3.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "keeping important information and leave irrelavant information\n",
    "\n",
    "3 problems of LSTM and RNNs\n",
    "\n",
    "- sequential computation cause parallelization\n",
    "- long and short range dependencies\n",
    "- distance is linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c659162",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "\n",
    "<img src='t4.gif' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8145cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paying attention to specific words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba078ca",
   "metadata": {},
   "source": [
    "<img src='t5.gif' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1cc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18cc2858",
   "metadata": {},
   "source": [
    "<img src='t6.gif' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699f35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a53e7bf5",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "<img src='t7.gif' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a624ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "it works in parallel\n",
    "- each word on the input can be processed at the same time\n",
    "and does not depends on the previous word\n",
    "\n",
    "\n",
    "CNN and attention\n",
    "\n",
    "the problem of dependencies will not be solved with it\n",
    "\n",
    "thats why they came up with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a9fb0",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "<img src='t8.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "the key innovation in transformers is self-attention\n",
    "\n",
    "\n",
    "RNNs and LSTM -> the model process the input is sequential\n",
    "\n",
    "\n",
    "transformer = encoder + decoder\n",
    "\n",
    "encoder and decoders = several identical layers\n",
    "\n",
    "\n",
    "\n",
    "encoder ->\n",
    "\n",
    "each layer = 2 sub-layers\n",
    "\n",
    "2 sub-layers = a self-attention mechanism + a position wise fully connected feedforward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbdefe",
   "metadata": {},
   "source": [
    "<img src='t9.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder\n",
    "\n",
    "2 sub-layers = a self-attention layer + cross attention layer + a position wise fully connected feedforward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150d4b2",
   "metadata": {},
   "source": [
    "<img src='t10.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c8c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d75b9511",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "<img src='t11.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9456e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "self-attention/scaled dot-product attention\n",
    "\n",
    "- calculates the relevance of each word in the sequence to the current word that being processed\n",
    "\n",
    "each input of the self attention is split into 3 transformations\n",
    "- query\n",
    "- key\n",
    "- value\n",
    "\n",
    "\n",
    "attention score\n",
    "\n",
    "softmax activation function - it normalize the attention scores within a sum to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Positional Encoding\n",
    "\n",
    "the position of the word in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef1308",
   "metadata": {},
   "source": [
    "<img src='t12.png' />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "the apple fruit is red in color. It is good for health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is self attention\n",
    "\n",
    "- assigning weigts to different parts of the sentence just to focus on the importance\n",
    "- scores based on the content of each position in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92bfee",
   "metadata": {},
   "source": [
    "# Hugging Face Models and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d249a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install tokenizers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd916c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging face give you 2 main libraries\n",
    "\n",
    "- transformers - text data\n",
    "- diffusers - image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6018d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\himan\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0a65e6d2034cacbcb0e85f3fc4bbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\himan\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fee4da0a114410885e757ed1f564ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ec965f665c48558603bf3f8228107e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e018d4b19b47d98a8ebbff6e844d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9996176958084106}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained sentiment analysis model\n",
    "sentiment_analysis = pipeline(\n",
    "\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "input_text = [\n",
    "\"Itâ€™s a great app, my biggest problem is the card readers regularly do not connect. Which is very poor customer service for us because we have to manually enter our customers debit cards, which takes time. This slows down our efficiency.\"\n",
    "]\n",
    "\n",
    "# Perform sentiment analysis on the input text\n",
    "result = sentiment_analysis(input_text)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718524de",
   "metadata": {},
   "source": [
    "## Microsoft DialoGPT-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0683629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62651ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbacd47984214c27a96fdc2ab5246b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\himan\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2acae496f574806b2ccbcdb63429548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffd19a4095f4189923e82eef4d921ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cae546362e24d4db9ca29ca168d9e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df22c6630ba429d98fef8b9361a2246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157416e2723d4092984834a9f883d861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_model = \"microsoft/DialoGPT-medium\"\n",
    "max_length = 1000\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a8308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_hf = \"If dinosaurs were alive today, would they possess a threat to people?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07a009b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: If dinosaurs were alive today, would they possess a threat to people?\n",
      "\n",
      "microsoft/DialoGPT-medium's Response: \n",
      "I think they would be more afraid of the humans.\n"
     ]
    }
   ],
   "source": [
    "user_input_ids = tokenizer.encode(user_prompt_hf + tokenizer.eos_token, return_tensors='pt')\n",
    "response_hf_encoded = model.generate(user_input_ids,\n",
    "                             max_length=max_length,\n",
    "                             pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "response_hf = tokenizer.decode(response_hf_encoded[:, user_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\n\\nPrompt: {user_prompt_hf}\\n\\n{hf_model}'s Response: \\n{response_hf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d215b5",
   "metadata": {},
   "source": [
    "### Questions answering web application with flask\n",
    "\n",
    "https://medium.com/@anoopjohny2000/building-a-question-answering-web-application-with-flask-and-transformers-9af2f69ef33d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6502b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
